---
title: "Down the Rabbit hole"
excerpt: "Distractions in bioinformatics"
header:
  teaser: /assets/images/2022-01-20/rabbit-hole.jpeg
toc: false
---

Sometimes in bioinformatics we end up going down rabbit holes and losing sight of the original goal. I caught myself doing just this recently: As I was going through the long process of assemblying the genome of *Stichopus chloronotus* I found myself on a tangent so far removed from the original goal that I started laughing. It's easy to get stuck in the weeds of bioinformatics and forget our original intentions, so here's an example of just that. 

I recently stumbled across a newly uploaded sea cucumber genome on NCBI that utilized Hi-C to acheive chromosome scale scaffolding. This made me excited as I was wanting to give an example of how to do this using sea cucumber data, but up until this genome was uploaded, there was no data available. So I was quite excited when I discovered the data. I downloaded the raw Hi-C, Pacbio, and 10X-chromium data and started  working on assemblying a genome. 

Prepping Hi-C data for genome scaffolding requires mapping the raw Hi-C reads to the genome assembly. Once mapped, the reads can be filtered based on how they mapped. This filtering step requires knowing what restrictions enzymes or Hi-C prep kit was used. If the restriction enzyme is known, then the scaffolding/filtering tool can look for mapped reads that have the restriction cut site at the end - simple right? However, in my experience of looking for Hi-C data sets, few people actually include in their papers or in meta-data what Hi-C kit or restriction enzymes was used. This is problematic and goes against the scientific goal of study reproducability. Nevertheless, it is common. 

So how does someone figure out what restriction enzyme was used? The simpliest approach is to take a peak at the first few bases in the reads and look for commonly used restriction enzyme cut sites. For example, DpnII is a common enzyme for this and the cut site is GATC. So GATC should appear frequently in Hi-C data if DpnII was used. However, adapter contamination and sequecing errors can get in the way of this simple technique. 

A more thorough technique is to classify all the 4mers or 5mers in the Hi-C and look for enriched kmers and hopefully the restriction cut site should show up at the top. So I did this using the tool called [Meryl](https://github.com/marbl/meryl). Unfortunately, GATC was middle of the pack for kmer counts and the highest kmers counts were enriched for 'A'. I am not sure why as none matched any restriction enzymes, but my hunch is sequencing adapters. I haven't looked further into this though.

So that didn't work, the next angle was to filter the reads by mapping the raw data to the genome assembly. Using the command ```bwa mem -SP5M``` accomplishes this. See below for an explanation by the 4D-nucleome project [website](https://data.4dnucleome.org/resources/data-analysis/hi_c-processing-pipeline) of how these parameters accomplish this. 

- The -SP option is used to ensure the results are equivalent to that obtained by running bwa mem on each mate separately, while retaining the right formatting for paired-end reads. This option skips a step in bwa mem that forces alignment of a poorly aligned read given an alignment of its mate with the assumption that the two mates are part of a single genomic segment.   
- The -5 option is used to report the 5' portion of chimeric alignments as the primary alignment. In Hi-C experiments, when a mate has chimeric alignments, typically, the 5' portion is the position of interest, while the 3' portion represents the same fragment as the mate. For chimeric alignments, bwa mem reports two alignments: one of them is annotated as primary and soft-clipped, retaining the full-length of the original sequence. The other end is annotated as hard-clipped and marked as either 'supplementary' or 'secondary'. 
- The -5 option forces the 5'end to be always annotated as primary.   
- The -M option is used to annotate the secondary/supplementary clipped reads as secondary rather than supplementary, for compatibility with some public software tools such as picard MarkDuplicates.

Before I mapped the reads to the genome though I wanted to make sure that the genome assembly had as few errors as possible. I have found that long read assemblies are riddled with various insertions and deletions, these tend to be sequencing platform and assembler specific and if they aren't corrected can greatly impact gene prediction and read mapping. So before mapping reads I wanted to polish the assembly using the long reads. So I tried using [hapo-g](https://github.com/institut-de-genomique/HAPO-G), but this maxed out my server ram and froze it. So I tried [Nextpolish](https://github.com/Nextomics/NextPolish) next and was running into dependency/installation issues.

When accessing whether I want to use a particular tool I often look at how how well maintained it is and how easy it is to install, and if the authors respond to github issues. If the github respository has not had any updates in more than a few years this tells me it isn't maintained and that I will likely run into issues running the program. I also apply this to installation, if it requires manually compiling the code and hunting missing dependencies then there is a good chance it will be a pain in the ass to install. This can further be compounded by the authors not offering support or responding to open github issues in their repository. So I will often abandon trying to use a tool if it begins to feel like I am fighting the software and it would easier to find a different tool that is equally as good or try a different approach to the problem at hand. 

At this point I was kinda annoyed and decided I could try using [Consent](https://github.com/morispi/CONSENT) to error correct the long reads. This didn't work as the conda version of Consent was throwing a invalid pointer error. So I sighed and decided to just take a solid look at the pacbio long read quality. This is when I discovered pacbio long read fastq files do not have phred quality scores as Pacbio decided phred quality scores were pointless and omitted this information when calling consensus reads. Ok, so if I really wanted quality metrics for pacbio I could either try calling consensus reads from the subreads.bam using other tools such as [deepconsensus](https://github.com/google/deepconsensus) or use a raw data qc tool for pacbio called [SequelTools](https://github.com/ISUgenomics/SequelTools). Suffice to say I had problems installing/using deepconsensus and SequelTools. 

I sat down at my desk and took a deep breath a few days ago and came to the realiziation I had strayed quite a long ways from my initial project of using Hi-C to scaffold a genome assembly. I refocused, figured out the dependency issue with NextPolish and reduced how much data I was using with Hapo-G. I successfully polished the genome and am now mapping Hi-C reads to the assembly with the goal of figuring out if I can use the mapped reads to ascertain the restriction enzymes used and move on to scaffolding. 

In summary, it is important to stop and ask yourself what your goal is and why you are doing such and such analysis. Is this helping you acheive that goal or is it a rabbit hole that you dove down. 
